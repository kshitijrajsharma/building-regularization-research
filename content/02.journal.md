Geographic information systems (GIS) and cartographic applications typically require building footprints as
precise vector polygons, rather than raster masks [@url:https://openaccess.thecvf.com/content/CVPR2022/html/Zorzi_PolyWorld_Polygonal_Building_Extraction_With_Graph_Neural_Networks_in_Satellite_CVPR_2022_paper.html]. Building footprint regularization refers to the process
of refining raw building outlines (e.g. from remotely sensed imagery or LiDAR) into clean polygon shapes that conform to
expected geometric constraints (such as orthogonal corners or aligned edges). The goal is to eliminate
irregular artifacts (noisy jags, misalignments) while preserving the true shape, so that the footprints are
cartographically suitable for maps
In many cases, regularization assumes buildings are rectilinear structures with predominantly 90° corners : an assumption that, while not universally true, holds for
most residential and industrial buildings. This review traces the evolution of footprint regularization
methods from early vectorization algorithms in the 1990s through modern deep learning approaches in the
2020s.

We focus on 2D footprint outline techniques (planimetric building outlines) and exclude full 3D
building reconstruction or roof modeling. Key developments and representative methods are discussed for
each era, highlighting their algorithms, use cases, strengths, and limitations. We then compare traditional
versus deep learning-based methods in terms of performance, flexibility, accuracy, and integration into GIS
workflows. The review draws on peer-reviewed research and real-world implementations (including open-
source tools and commercial pipelines) to provide a comprehensive perspective for remote sensing and GIS
professionals.

## Geometric and Heuristic Methods ( 1990s - 2000s )
 
**Edge Detection and Line Fitting**: Early building extraction in the 1990s relied on low-level image processing and geometric heuristics. For example, Huertas and Nevatia (1988) developed a system to detect buildings in aerial images by finding rectangular clusters of edges (lines) and using shadow cues to distinguish buildings from other structures [@doi:10.3390/ijgi8040191] . Building polygons often consist of jagged lines. Guercke and Sester [@url:https://scholar.google.com/scholar_lookup?title=Building+Footprint+Simplification+Based+on+Hough+Transform+and+Least+Squares+Adjustment&conference=Proceedings+of+the+14th+Workshop+of+the+ICA+Commission+on+Generalisation+and+Multiple+Representation&author=Guercke,+R.&author=Sester,+M.&publication_year=2011] use Hough-Transformation to refine such polygons.

Those approach and similar ones could identify simple rectangular building footprints, but often produced polygons with jagged (bearing in mind they don't take into account the building shape itself rather the outline), noisy outlines. To clean such outlines, researchers applied line simplification algorithms from cartography, notably the Ramer–Douglas–Peucker algorithm : to remove small zig-zags and reduce vertex count while approximating the shape (which is still used to the date) [@url:https://element84.com/software-engineering/automated-building-footprint-extraction-part-3-model-architectures/].

The Douglas–Peucker algorithm (originally from 1973) became a common post-processing step to “compress” or simplify building polygon geometry.
![A simple illustration of Douglas-Peucker algorithm](https://github.com/user-attachments/assets/d8a3f362-6fd0-4dfb-84e8-a686275c82c5){#fig:douglas-peucker}
Overall, early methods were largely rule-based: edges and corners were detected via image filters, and building shapes were assembled by connecting these primitives under geometric constraints defined by human experts.


